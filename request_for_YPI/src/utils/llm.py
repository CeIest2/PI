# src/utils/llm.py
import os
from langchain_mistralai import ChatMistralAI

def get_llm(mode_or_model: str = "smart"):
    """
    Renvoie le mod√®le Mistral configur√© selon le mode demand√©.
    
    MODES :
    - 'fast'      : Utilise mistral-small (Rapide, parfait pour le r√©sum√©/scraping).
    - 'smart'     : Utilise mistral-large (Polyvalent, pour la r√©daction finale).
    - 'reasoning' : Utilise magistral (Le mod√®le de raisonnement avanc√©).
    """
    
    # --- CONFIGURATION DES MOD√àLES MISTRAL ---
    # Noms officiels de l'API Mistral
    MODEL_FAST = "mistral-small-latest"
    MODEL_SMART = "mistral-large-latest"
    MODEL_REASONING = "magistral-medium-latest" # Ou 'magistral-small-latest' selon votre acc√®s
    
    # R√©cup√©ration de la cl√© API
    api_key = os.getenv("MISTRAL_API_KEY")
    if not api_key:
        print("‚ö†Ô∏è Warning: MISTRAL_API_KEY introuvable dans les variables d'environnement.")

    print(f"üß† [LLM Load] Mode: {mode_or_model}")

    # --- S√âLECTION DU MOD√àLE ---
    
    if mode_or_model == "fast":
        return ChatMistralAI(
            model=MODEL_FAST,
            temperature=0,
            mistral_api_key=api_key
        )
        
    elif mode_or_model == "smart":
        return ChatMistralAI(
            model=MODEL_SMART,
            temperature=0.2, # L√©g√®re cr√©ativit√© pour la r√©daction
            mistral_api_key=api_key
        )
        
    elif mode_or_model == "reasoning":
        print(f"   ‚Ü≥ ‚ú® Activation du mode Raisonnement ({MODEL_REASONING})")
        return ChatMistralAI(
            model=MODEL_REASONING,
            temperature=0, # Temp√©rature 0 recommand√©e pour les t√¢ches logiques pures
            mistral_api_key=api_key
        )

    # Fallback : Si on passe un nom de mod√®le direct (ex: "open-mixtral-8x22b")
    else:
        return ChatMistralAI(
            model=mode_or_model,
            temperature=0,
            mistral_api_key=api_key
        )