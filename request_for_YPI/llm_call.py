from mistralai import Mistral
import os




# function to call a llm and output the respond

import os
from mistralai import Mistral

# Assure-toi d'avoir install√© le package : pip install mistralai

def llm_call_respond(prompt: str, thinking: bool = True) -> str:

    
    # 1. V√©rification de la Cl√© API
    api_key = os.environ.get("MISTRAL_API_KEY")
    if not api_key:
        print("‚ùå Erreur : La variable d'environnement MISTRAL_API_KEY n'est pas d√©finie.")
        return "Erreur : Pas de cl√© API trouv√©e."
    
    client = Mistral(api_key=api_key)

    if thinking:
        models_candidates = [
            "magistral-medium-latest", 
            "magistral-small-latest"
        ]
    else:
        models_candidates = [
            "mistral-large-latest",
            "mistral-large-2512",
            "mistral-large-2411",
            "mistral-medium-latest"
        ]

    last_error = None
    
    for model in models_candidates:
        print(f"üîÑ Tentative avec le mod√®le : {model}...")
        
        try:
            response = client.chat.complete(
                model=model,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            print(f"‚úÖ Succ√®s ! Mod√®le utilis√© : {model}")
            raw_content = response.choices[0].message.content
            print("####")
            print("#####")
            print(f"üí¨ Contenu brut re√ßu : {raw_content}...")
            print("####")
            print("#####")
            # Si le contenu est une liste (Cas des mod√®les avec Reasoning)
            if isinstance(raw_content, list):
                final_text = ""
                for chunk in raw_content:
                    # On v√©rifie si le chunk est de type 'text'
                    # (On ignore les 'thinking' ou 'reasoning')
                    if hasattr(chunk, 'type') and chunk.type == 'text':
                        final_text += chunk.text
                return final_text
            
        except Exception as e:
            print(f"‚ö†Ô∏è √âchec avec {model}. Raison : {e}")
            last_error = e
            continue # Passe au mod√®le suivant dans la liste

    error_msg = f"‚ùå Tous les mod√®les ont √©chou√©. Derni√®re erreur : {last_error}"
    print(error_msg)
    return error_msg
        


if __name__ == "__main__":
    test_prompt = "Explique la th√©orie de la relativit√© en termes simples."
    response = llm_call_respond(test_prompt, thinking=False)
    print("\nüí¨ R√©ponse du LLM :")
    print(response)